{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "import time\n",
    "# nltk.download('punkt')\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            data.append({\n",
    "                'id': row['id'],\n",
    "                'article': row['article'],\n",
    "                'highlights': row['highlights']\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_csv('./Cleaned_Dataset/train.csv')\n",
    "test_data = load_csv('./Cleaned_Dataset/test.csv')\n",
    "val_data = load_csv('./Cleaned_Dataset/validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    pattern = r\"(?i)(PUBLISHED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4}\\s*.\\s*\\|\\s*.\\s*UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(By\\s*.\\s*[A-Za-z\\s]+.)|\" \\\n",
    "              r\"(\\([A-Za-z\\s]*CNN\\)\\s*--)|\" \\\n",
    "              r\"(Follow\\s*@@[A-Za-z0-9_]+)|\" \\\n",
    "              r\"(UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(Last\\s*updated\\s*at\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*on\\s*\\d{1,2}(st|nd|rd|th)\\s*\\w+\\s\\d{4}\\s*.)|\" \\\n",
    "              r\"(\\(CNN\\))\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text).strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_articles(data):\n",
    "    for entry in data:\n",
    "        entry['article'] = clean_text(entry['article'])\n",
    "        entry['highlights'] = clean_text(entry['highlights'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def write_csv(file_path, cleaned_data):\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer=writer\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in cleaned_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# train_data = clean_articles(train_data)\n",
    "# test_data = clean_articles(test_data)\n",
    "# val_data = clean_articles(val_data)\n",
    "\n",
    "# write_csv(\"./Cleaned_Dataset/train.csv\", train_data)\n",
    "# write_csv(\"./Cleaned_Dataset/test.csv\", test_data)\n",
    "# write_csv(\"./Cleaned_Dataset/validation.csv\", val_data)\n",
    "            \n",
    "def write_csv(file_path, cleaned_data, percentage=1):\n",
    "    # Calculate how many rows to write based on the percentage\n",
    "    data_size = len(cleaned_data)\n",
    "    num_rows = data_size * percentage // 100\n",
    "\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write only the first 'num_rows' rows of the data\n",
    "        for row in cleaned_data[:num_rows]:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Assuming train_data, test_data, val_data are your datasets\n",
    "# write_csv(\"./train.csv\", train_data)\n",
    "# write_csv(\"./test.csv\", test_data)\n",
    "# write_csv(\"./validation.csv\", val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intialize special Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Initializing Pad tokens\n",
    "pad_token = tokenizer.eos_token_id\n",
    "# tokenizer.add_tokens([pad_token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCSV(data):\n",
    "    inp = []\n",
    "    out = []\n",
    "    for row in data:\n",
    "        inp.append(row['article'])\n",
    "        out.append(row['highlights'])\n",
    "    \n",
    "    return inp, out\n",
    "\n",
    "inp_train, out_train = convertCSV(train_data)\n",
    "inp_val, out_val = convertCSV(val_data)\n",
    "inp_test, out_test = convertCSV(test_data)\n",
    "\n",
    "train_size = int(0.05 * len(inp_train))\n",
    "inp_train_10 = inp_train[:train_size]\n",
    "out_train_10 = out_train[:train_size]\n",
    "\n",
    "val_size = int(0.05 * len(inp_val))\n",
    "inp_val_10 = inp_val[:val_size]\n",
    "out_val_10 = out_val[:val_size]\n",
    "\n",
    "test_size = int(0.05 * len(inp_test))\n",
    "inp_test_10 = inp_test[:test_size]\n",
    "out_test_10 = out_test[:test_size]\n",
    "\n",
    "# print(inp_train[0])\n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "#Using NLTK Tokenize\n",
    "\n",
    "inp_train = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_train_10]\n",
    "inp_test = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_test_10]\n",
    "inp_val = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_val_10]\n",
    "out_train = [remove_punctuation(word_tokenize(sentence)) for sentence in out_train_10]\n",
    "out_test = [remove_punctuation(word_tokenize(sentence)) for sentence in out_test_10]\n",
    "out_val = [remove_punctuation(word_tokenize(sentence)) for sentence in out_val_10]\n",
    "\n",
    "print(len(inp_train))\n",
    "print(len(inp_val))\n",
    "print(len(inp_test))\n",
    "# print(out_train[0])\n",
    "\n",
    "max_len = 0\n",
    "for i in inp_train:\n",
    "    # if max_len < len(i):\n",
    "    max_len += len(i)\n",
    "    \n",
    "print(max_len/len(inp_train))\n",
    "\n",
    "# def tokenize(data,max_len = 1000):\n",
    "\n",
    "def prepare_data(sentences,pad, max_len=1024):\n",
    "    all_indices = []\n",
    "    for _, sentence in enumerate(sentences):\n",
    "\n",
    "        tokens = tokenizer.encode(sentence,truncation=True,max_length=max_len)\n",
    "        padded_tokens = torch.tensor(tokens + [pad] * (max_len - len(tokens)))\n",
    "        \n",
    "        all_indices.append(padded_tokens)\n",
    "        \n",
    "    return all_indices\n",
    "\n",
    "\n",
    "train_inp = prepare_data(inp_train,pad_token,512)\n",
    "test_inp = prepare_data(inp_test,pad_token,512)\n",
    "val_inp = prepare_data(inp_val,pad_token,512)\n",
    "train_out = prepare_data(out_train,pad_token,512)\n",
    "test_out = prepare_data(out_test,pad_token, 512)\n",
    "val_out = prepare_data(out_val,pad_token,512)\n",
    "# print(train_inp[0])\n",
    "# print(train_out[0][1023])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SummarizationFineTune(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.gpt2.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # for param in self.gpt2.transformer.h[-1].parameters():\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        if labels is not None:\n",
    "            outputs = self.gpt2(input_ids=input_ids, labels=labels)\n",
    "        else:\n",
    "            outputs = self.gpt2(input_ids=input_ids)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paramters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "old_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "for param in old_model.parameters():\n",
    "            param.requires_grad = False\n",
    "model = GPT2SummarizationFineTune(\"gpt2\")\n",
    "model = model.to(device)\n",
    "\n",
    "old_params = count_parameters(old_model)\n",
    "new_params = count_parameters(model)\n",
    "added_params = new_params - old_params\n",
    "print(\"Trainable params in the model: \", added_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rougeL = 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return average_rouge1, average_rougeL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 1e-4\n",
    "accumulation_steps = 8\n",
    "clip_value = 1.0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_rouge1_scores = []\n",
    "    train_rouge2_scores = []\n",
    "    train_rougeL_scores = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(enumerate(zip(train_inp, train_out)), total=len(train_inp), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as progress:\n",
    "        train_percentage_matched = 0\n",
    "        train_percentage_matched_ct = 0\n",
    "        for i, (article, summary) in progress:\n",
    "            context_words = article.to(device)\n",
    "            target_words = summary.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(context_words, labels=target_words)\n",
    "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "            loss = criterion(outputs.logits, target_words)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if( not math.isnan(loss.item())):\n",
    "                total_loss += loss.item()  \n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0 or (i+1) == len(train_inp):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**2 \n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "            progress.set_postfix(\n",
    "                curr_loss=loss.item(),\n",
    "                gpu_allocated=f'{gpu_memory_allocated:.2f} MB',\n",
    "                gpu_reserved=f'{gpu_memory_reserved:.2f} MB'\n",
    "            )\n",
    "\n",
    "            predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "            references = summary.squeeze(0).tolist()\n",
    "\n",
    "            decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "            decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "            rouge1, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "            train_rouge1_scores.append(rouge1)\n",
    "            train_rougeL_scores.append(rougeL)\n",
    "\n",
    "        print(total_loss)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_inp)  # Use len(train_inp) for average\n",
    "        train_losses.append(avg_train_loss)\n",
    "        avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
    "        avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
    "\n",
    "        print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
    "        print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
    "\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    val_rouge1_scores = []\n",
    "    val_rougeL_scores = []\n",
    "\n",
    "    with tqdm(enumerate(zip(val_inp, val_out)), total=len(val_inp), desc=f\"Validation\", unit=\"batch\") as progress:\n",
    "        for _, (val_article, val_summary) in progress:\n",
    "            context_words = val_article.to(device)\n",
    "            target_words = val_summary.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context_words)\n",
    "                logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "                loss = criterion(logits, target_words)\n",
    "                total_val_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "                predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "                references = val_summary.squeeze(0).tolist()\n",
    "\n",
    "                decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "                decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "                rouge1, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "                val_rouge1_scores.append(rouge1)\n",
    "                val_rougeL_scores.append(rougeL)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_inp)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        avg_val_rouge1_score = sum(val_rouge1_scores) / len(val_rouge1_scores)\n",
    "        avg_val_rougeL_score = sum(val_rougeL_scores) / len(val_rougeL_scores)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(\"Average Validation ROUGE-1 Score:\", avg_val_rouge1_score)\n",
    "        print(\"Average Validation ROUGE-L Score:\", avg_val_rougeL_score)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Time taken to train is: \", training_time)\n",
    "peak_memory_allocated = torch.cuda.max_memory_allocated() / 1024**2\n",
    "print(f'Peak GPU memory allocated: {peak_memory_allocated:.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(\"./plot_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "total_test_loss = 0\n",
    "test_losses = []\n",
    "test_rouge1_scores = []\n",
    "test_rougeL_scores = []\n",
    "\n",
    "with tqdm(enumerate(zip(test_inp, test_out)), total=len(test_inp), desc=\"Testing\", unit=\"batch\") as progress:\n",
    "    for _, (test_article, test_summary) in progress:\n",
    "        context_words = test_article.to(device)\n",
    "        target_words = test_summary.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(context_words)\n",
    "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "            loss = criterion(logits, target_words)\n",
    "            total_test_loss += loss.item() \n",
    "\n",
    "            predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "            references = test_summary.squeeze(0).tolist()\n",
    "\n",
    "            decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "            decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "            rouge1, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "            test_rouge1_scores.append(rouge1)\n",
    "            test_rougeL_scores.append(rougeL)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_inp)\n",
    "test_losses.append(avg_test_loss)\n",
    "\n",
    "avg_test_rouge1_score = sum(test_rouge1_scores) / len(test_rouge1_scores)\n",
    "avg_test_rougeL_score = sum(test_rougeL_scores) / len(test_rougeL_scores)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(\"Average Test ROUGE-1 Score:\", avg_test_rouge1_score)\n",
    "print(\"Average Test ROUGE-L Score:\", avg_test_rougeL_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text = \"the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it's important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\"\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1023)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "\n",
    "\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "predicted_tokens = tokenizer.decode([token for token in predicted_token_ids if token != pad_token])\n",
    "print(predicted_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
