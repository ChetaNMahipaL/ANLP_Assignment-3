{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chetan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# import tokenizer for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            data.append({\n",
    "                'id': row['id'],\n",
    "                'article': row['article'],\n",
    "                'highlights': row['highlights']\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_csv('./Cleaned_Dataset/train.csv')\n",
    "test_data = load_csv('./Cleaned_Dataset/test.csv')\n",
    "val_data = load_csv('./Cleaned_Dataset/validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    pattern = r\"(?i)(PUBLISHED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4}\\s*.\\s*\\|\\s*.\\s*UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(By\\s*.\\s*[A-Za-z\\s]+.)|\" \\\n",
    "              r\"(\\([A-Za-z\\s]*CNN\\)\\s*--)|\" \\\n",
    "              r\"(Follow\\s*@@[A-Za-z0-9_]+)|\" \\\n",
    "              r\"(UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(Last\\s*updated\\s*at\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*on\\s*\\d{1,2}(st|nd|rd|th)\\s*\\w+\\s\\d{4}\\s*.)|\" \\\n",
    "              r\"(\\(CNN\\))\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text).strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_articles(data):\n",
    "    for entry in data:\n",
    "        entry['article'] = clean_text(entry['article'])\n",
    "        entry['highlights'] = clean_text(entry['highlights'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def write_csv(file_path, cleaned_data):\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer=writer\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in cleaned_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# train_data = clean_articles(train_data)\n",
    "# test_data = clean_articles(test_data)\n",
    "# val_data = clean_articles(val_data)\n",
    "\n",
    "# write_csv(\"./Cleaned_Dataset/train.csv\", train_data)\n",
    "# write_csv(\"./Cleaned_Dataset/test.csv\", test_data)\n",
    "# write_csv(\"./Cleaned_Dataset/validation.csv\", val_data)\n",
    "            \n",
    "def write_csv(file_path, cleaned_data, percentage=1):\n",
    "    # Calculate how many rows to write based on the percentage\n",
    "    data_size = len(cleaned_data)\n",
    "    num_rows = data_size * percentage // 100\n",
    "\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write only the first 'num_rows' rows of the data\n",
    "        for row in cleaned_data[:num_rows]:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Assuming train_data, test_data, val_data are your datasets\n",
    "write_csv(\"./train.csv\", train_data)\n",
    "write_csv(\"./test.csv\", test_data)\n",
    "write_csv(\"./validation.csv\", val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intialize special Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokken = \"[SUMMARIZE]\"\n",
    "soft_prompt_vocab = [\"[SUMMARIZE]\"]\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in prompt_tokken.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in prompt_tokken.split()])\n",
    "# print(prompt_id)\n",
    "\n",
    "# Initializing Pad tokens\n",
    "pad_token = tokenizer.eos_token_id\n",
    "# tokenizer.add_tokens([pad_token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'bishop', 'of', 'the', 'fargo', 'catholic', 'diocese', 'in', 'north', 'dakota', 'has', 'exposed', 'potentially', 'hundreds', 'of', 'church', 'members', 'in', 'fargo', 'grand', 'forks', 'and', 'jamestown', 'to', 'the', 'hepatitis', 'a', 'virus', 'in', 'late', 'september', 'and', 'early', 'october', 'the', 'state', 'health', 'department', 'has', 'issued', 'an', 'advisory', 'of', 'exposure', 'for', 'anyone', 'who', 'attended', 'five', 'churches', 'and', 'took', 'communion', 'bishop', 'john', 'folda', 'pictured', 'of', 'the', 'fargo', 'catholic', 'diocese', 'in', 'north', 'dakota', 'has', 'exposed', 'potentially', 'hundreds', 'of', 'church', 'members', 'in', 'fargo', 'grand', 'forks', 'and', 'jamestown', 'to', 'the', 'hepatitis', 'a', 'state', 'immunization', 'program', 'manager', 'molly', 'howell', 'says', 'the', 'risk', 'is', 'low', 'but', 'officials', 'feel', 'it', \"'s\", 'important', 'to', 'alert', 'people', 'to', 'the', 'possible', 'exposure', 'the', 'diocese', 'announced', 'on', 'monday', 'that', 'bishop', 'john', 'folda', 'is', 'taking', 'time', 'off', 'after', 'being', 'diagnosed', 'with', 'hepatitis', 'a.', 'the', 'diocese', 'says', 'he', 'contracted', 'the', 'infection', 'through', 'contaminated', 'food', 'while', 'attending', 'a', 'conference', 'for', 'newly', 'ordained', 'bishops', 'in', 'italy', 'last', 'month', 'symptoms', 'of', 'hepatitis', 'a', 'include', 'fever', 'tiredness', 'loss', 'of', 'appetite', 'nausea', 'and', 'abdominal', 'discomfort', 'fargo', 'catholic', 'diocese', 'in', 'north', 'dakota', 'pictured', 'is', 'where', 'the', 'bishop', 'is', 'located']\n",
      "['bishop', 'john', 'folda', 'of', 'north', 'dakota', 'is', 'taking', 'time', 'off', 'after', 'being', 'diagnosed', 'he', 'contracted', 'the', 'infection', 'through', 'contaminated', 'food', 'in', 'italy', 'church', 'members', 'in', 'fargo', 'grand', 'forks', 'and', 'jamestown', 'could', 'have', 'been', 'exposed']\n",
      "657.6271777003484\n",
      "tensor([ 1169, 27832,  1659,  1169, 50256, 50256, 50256,   259, 43588, 50256,\n",
      "        10134, 50256, 50256, 50256,  1659, 36964, 30814,   259, 50256, 23936,\n",
      "        50256,   392, 50256,  1462,  1169, 50256,    64, 50256,   259, 17660,\n",
      "        50256,   392, 11458, 50256,  1169,  5219, 13948, 50256, 10134, 39361,\n",
      "          272, 50256,  1659, 50256,  1640, 50256,  8727, 50256, 13261, 50256,\n",
      "          392, 50256, 50256, 27832, 30686, 50256, 28852,  1659,  1169, 50256,\n",
      "        50256, 50256,   259, 43588, 50256, 10134, 50256, 50256, 50256,  1659,\n",
      "        36964, 30814,   259, 50256, 23936, 50256,   392, 50256,  1462,  1169,\n",
      "        50256,    64,  5219, 50256, 23065, 37153, 50256, 50256, 50256,  1169,\n",
      "        19121,   271,  9319,  4360, 50256, 36410,   270,   338, 18049,  1462,\n",
      "        44598, 15332,  1462,  1169, 50256, 50256,  1169, 50256, 43499,   261,\n",
      "        50256,  5562, 27832, 30686, 50256,   271, 26103,  2435,  2364,  8499,\n",
      "        11873, 50256,  4480, 50256, 50256,  1169, 50256, 50256,   258, 50256,\n",
      "         1169, 50256,  9579, 50256, 19425,  4514, 50256,    64, 41124,  1640,\n",
      "        50256, 50256, 50256,   259, 50256, 12957,  8424, 50256,  1659, 50256,\n",
      "           64, 17256, 50256, 50256, 22462,  1659, 50256, 50256,   392, 50256,\n",
      "        50256, 50256, 50256, 50256,   259, 43588, 50256, 28852,   271,  3003,\n",
      "         1169, 27832,   271, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256])\n"
     ]
    }
   ],
   "source": [
    "def convertCSV(data):\n",
    "    inp = []\n",
    "    out = []\n",
    "    for row in data:\n",
    "        inp.append(row['article'])\n",
    "        out.append(row['highlights'])\n",
    "    \n",
    "    return inp, out\n",
    "\n",
    "inp_train, out_train = convertCSV(train_data)\n",
    "inp_test, out_test = convertCSV(test_data)\n",
    "\n",
    "train_size = int(0.001 * len(inp_train))\n",
    "inp_train_10 = inp_train[:train_size]\n",
    "out_train_10 = out_train[:train_size]\n",
    "\n",
    "test_size = int(0.001 * len(inp_test))\n",
    "inp_test_10 = inp_test[:test_size]\n",
    "out_test_10 = out_test[:test_size]\n",
    "\n",
    "# print(inp_train[0])\n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "#Using NLTK Tokenize\n",
    "\n",
    "inp_train = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_train_10]\n",
    "inp_test = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_test_10]\n",
    "out_train = [remove_punctuation(word_tokenize(sentence)) for sentence in out_train_10]\n",
    "out_test = [remove_punctuation(word_tokenize(sentence)) for sentence in out_test_10]\n",
    "\n",
    "print(inp_train[0])\n",
    "print(out_train[0])\n",
    "\n",
    "max_len = 0\n",
    "for i in inp_train:\n",
    "    # if max_len < len(i):\n",
    "    max_len += len(i)\n",
    "    \n",
    "print(max_len/len(inp_train))\n",
    "\n",
    "# def tokenize(data,max_len = 1000):\n",
    "\n",
    "def prepare_data(sentences,pad, max_len=1024):\n",
    "    all_indices = []\n",
    "    for _, sentence in enumerate(sentences):\n",
    "\n",
    "        tokens = tokenizer.encode(sentence,truncation=True,max_length=max_len)\n",
    "        padded_tokens = torch.tensor(tokens + [pad] * (max_len - len(tokens)))\n",
    "        \n",
    "        all_indices.append(padded_tokens)\n",
    "        \n",
    "    return all_indices\n",
    "\n",
    "\n",
    "train_inp = prepare_data(inp_train,pad_token,512-num_prompts)\n",
    "test_inp = prepare_data(inp_test,pad_token,512-num_prompts)\n",
    "train_out = prepare_data(out_train,pad_token,512)\n",
    "test_out = prepare_data(out_test,pad_token, 512)\n",
    "print(train_inp[0])\n",
    "# print(train_out[0][1023])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model, num_prompts, emb_size = 768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model)\n",
    "        self.gpt2.eval()  # Freeze GPT-2 weights\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.sprompt = torch.nn.Embedding(num_prompts, emb_size)\n",
    "\n",
    "    def forward(self, inp, prompt):\n",
    "        prompt_emb = self.sprompt(prompt)\n",
    "        gpt2_emb = self.gpt2.transformer.wte(inp)\n",
    "        emb = torch.cat([prompt_emb, gpt2_emb.squeeze(0)], dim=0)\n",
    "        attention_mask = torch.ones((emb.size(0), emb.size(1)), device=emb.device)\n",
    "        outputs = self.gpt2(inputs_embeds=emb, attention_mask=attention_mask)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return average_rouge1, average_rouge2, average_rougeL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 287/287 [00:36<00:00,  7.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training ROUGE-1 Score: 0.00031050819393800186\n",
      "Average Training ROUGE-2 Score: 0.0\n",
      "Average Training ROUGE-L Score: 0.00031050819393800186\n",
      "Epoch [1/1], Train Loss: 8.6895,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2SoftPrompt(\"gpt2\", num_prompts)\n",
    "\n",
    "# # Freeze GPT-2 model weights\n",
    "# for param in model.gpt2_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 0.01\n",
    "clip_value = 1.0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "prompt_id = prompt_id.to(device)\n",
    "train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_rouge1_scores = []\n",
    "    train_rouge2_scores = []\n",
    "    train_rougeL_scores = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(enumerate(zip(train_inp, train_out)), total=len(train_inp), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as progress:\n",
    "        train_percentage_matched = 0\n",
    "        train_percentage_matched_ct = 0\n",
    "        for _, (article, summary) in progress:\n",
    "            context_words = article.to(device)\n",
    "            target_words = summary.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(context_words, prompt_id)\n",
    "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            # outputs = outputs.view(-1, outputs.size(-1))\n",
    "            # target_words_out = target_words[:, 1:].view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.logits, target_words)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # Convert tensor predictions and references to lists\n",
    "            predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "            references = summary.squeeze(0).tolist()\n",
    "\n",
    "            # Decode predictions and references, ignoring pad tokens\n",
    "            decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "            decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "            rouge1, rouge2, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "            train_rouge1_scores.append(rouge1)\n",
    "            train_rouge2_scores.append(rouge2)\n",
    "            train_rougeL_scores.append(rougeL)\n",
    "\n",
    "\n",
    "        # # Calculate average training loss\n",
    "        avg_train_loss = total_loss / len(train_inp)  # Use len(train_inp) for average\n",
    "        train_losses.append(avg_train_loss)\n",
    "        avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
    "        avg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\n",
    "        avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
    "\n",
    "        print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
    "        print(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\n",
    "        print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
    "\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        # model.eval()\n",
    "        # total_val_loss = 0\n",
    "        # correct = 0\n",
    "        # total = 0\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for batch in dataloader_val:\n",
    "        #         context_words, target_words = batch\n",
    "        #         context_words = context_words.to(device)\n",
    "        #         target_words = target_words.to(device)\n",
    "\n",
    "        #         outputs = model(context_words, target_words[:, :-1])\n",
    "\n",
    "        #         outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "        #         target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        #         loss = criterion(outputs, target_words_out)\n",
    "\n",
    "        #         total_val_loss += loss.item()\n",
    "\n",
    "        #         _, predicted = torch.max(outputs, 1)\n",
    "        #         total += target_words_out.size(0)\n",
    "        #         correct += (predicted == target_words_out).sum().item()\n",
    "\n",
    "\n",
    "        # avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "        # val_losses.append(avg_val_loss)\n",
    "        # accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},')\n",
    "            #    Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookthe-thetheandthethethethe caththe-resoinin thein ofdmembersin theargo caththed, the-ins thefca, the-ghe, the sea,\n",
      "j ofcare has exposed ac about the to the inhad the or in the the in the ofs and hasthe)of the fargo churcholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grandk and jamestown to the hepatitis a virus\n",
      " healthcarecare ininjc theof the state of in. and the are the's high. get the of the hepatitis..\n",
      " stateoceseof the theolly that it john folda of the communion to from the in with hepatitis. . the stateocese announced the's hepatitis hepatitis in the food and in church church. the- church. the.. year. the of hepatitis a.:, coldness, and of appetite, and, a pain. theargo isolic diocese in north dakota hasleft) of taking the hepatitisof the. the\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it's important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\"\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1023)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device),prompt_id)\n",
    "    pred_logits = outputs.logits\n",
    "    # print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode([token for token in predicted_token_ids if token != pad_token])\n",
    "print(predicted_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
