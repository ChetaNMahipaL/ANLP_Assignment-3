{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import csv\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            data.append({\n",
    "                'id': row['id'],\n",
    "                'article': row['article'],\n",
    "                'highlights': row['highlights']\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_csv('./Cleaned_Dataset/train.csv')\n",
    "test_data = load_csv('./Cleaned_Dataset/test.csv')\n",
    "val_data = load_csv('./Cleaned_Dataset/validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    pattern = r\"(?i)(PUBLISHED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4}\\s*.\\s*\\|\\s*.\\s*UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(By\\s*.\\s*[A-Za-z\\s]+.)|\" \\\n",
    "              r\"(\\([A-Za-z\\s]*CNN\\)\\s*--)|\" \\\n",
    "              r\"(Follow\\s*@@[A-Za-z0-9_]+)|\" \\\n",
    "              r\"(UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(Last\\s*updated\\s*at\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*on\\s*\\d{1,2}(st|nd|rd|th)\\s*\\w+\\s\\d{4}\\s*.)|\" \\\n",
    "              r\"(\\(CNN\\))\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text).strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_articles(data):\n",
    "    for entry in data:\n",
    "        entry['article'] = clean_text(entry['article'])\n",
    "        entry['highlights'] = clean_text(entry['highlights'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def write_csv(file_path, cleaned_data):\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer=writer\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in cleaned_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# train_data = clean_articles(train_data)\n",
    "# test_data = clean_articles(test_data)\n",
    "# val_data = clean_articles(val_data)\n",
    "\n",
    "# write_csv(\"./Cleaned_Dataset/train.csv\", train_data)\n",
    "# write_csv(\"./Cleaned_Dataset/test.csv\", test_data)\n",
    "# write_csv(\"./Cleaned_Dataset/validation.csv\", val_data)\n",
    "            \n",
    "# def write_csv(file_path, cleaned_data, percentage=1):\n",
    "#     # Calculate how many rows to write based on the percentage\n",
    "#     data_size = len(cleaned_data)\n",
    "#     num_rows = data_size * percentage // 100\n",
    "\n",
    "#     with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "#         writer.writeheader()\n",
    "        \n",
    "#         # Write only the first 'num_rows' rows of the data\n",
    "#         for row in cleaned_data[:num_rows]:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# # Assuming train_data, test_data, val_data are your datasets\n",
    "# write_csv(\"./train.csv\", train_data)\n",
    "# write_csv(\"./test.csv\", test_data)\n",
    "# write_csv(\"./validation.csv\", val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intialize special Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chetan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import tokenizer for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token': '[PAD]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'bos_token': '[BOS]',\n",
    "})\n",
    "\n",
    "# # Initializing Pad tokens\n",
    "# pad_token = tokenizer.eos_token_id\n",
    "# print(pad_token)\n",
    "# tokenizer.add_tokens([pad_token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCSV(data):\n",
    "    inp = []\n",
    "    out = []\n",
    "    for row in data:\n",
    "        inp.append(row['article'])\n",
    "        out.append(row['highlights'])\n",
    "    \n",
    "    return inp, out\n",
    "\n",
    "inp_train, out_train = convertCSV(train_data)\n",
    "inp_test, out_test = convertCSV(test_data)\n",
    "\n",
    "train_size = int(0.001 * len(inp_train))\n",
    "inp_train_10 = inp_train[:train_size]\n",
    "out_train_10 = out_train[:train_size]\n",
    "\n",
    "test_size = int(0.001 * len(inp_test))\n",
    "inp_test_10 = inp_test[:test_size]\n",
    "out_test_10 = out_test[:test_size]\n",
    "\n",
    "# print(inp_train[0])\n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "#Using NLTK Tokenize\n",
    "\n",
    "inp_train = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_train_10]\n",
    "inp_test = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_test_10]\n",
    "out_train = [remove_punctuation(word_tokenize(sentence)) for sentence in out_train_10]\n",
    "out_test = [remove_punctuation(word_tokenize(sentence)) for sentence in out_test_10]\n",
    "\n",
    "# print(inp_train[0])\n",
    "# print(out_train[0])\n",
    "\n",
    "max_len = 0\n",
    "for i in inp_train:\n",
    "    # if max_len < len(i):\n",
    "    max_len += len(i)\n",
    "    \n",
    "# print(max_len/len(inp_train))\n",
    "\n",
    "# def tokenize(data,max_len = 1000):\n",
    "def prepare_data(articles, summaries, max_len=1024):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for article, summary in zip(articles, summaries):\n",
    "        # Format: [BOS] Article [SEP] Summary [EOS]\n",
    "        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_len-150)\n",
    "        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=150)\n",
    "        \n",
    "        # Combine tokens with special tokens\n",
    "        combined_tokens = (\n",
    "            [tokenizer.bos_token_id] + \n",
    "            article_tokens + \n",
    "            [tokenizer.sep_token_id] + \n",
    "            summary_tokens + \n",
    "            [tokenizer.eos_token_id]\n",
    "        )\n",
    "        \n",
    "        # Ensure combined tokens don't exceed max_len\n",
    "        if len(combined_tokens) > max_len:\n",
    "            combined_tokens = combined_tokens[:max_len]\n",
    "        \n",
    "        # Pad sequence\n",
    "        padding_length = max_len - len(combined_tokens)\n",
    "        attention_mask = [1] * len(combined_tokens) + [0] * padding_length\n",
    "        combined_tokens = combined_tokens + [tokenizer.pad_token_id] * padding_length\n",
    "        \n",
    "        # Create labels: -100 for non-summary tokens\n",
    "        labels_array = (\n",
    "            [-100] * (len(article_tokens) + 2) +  # +2 for BOS and SEP tokens\n",
    "            summary_tokens +\n",
    "            [tokenizer.eos_token_id]\n",
    "        )\n",
    "        \n",
    "        # Ensure labels array matches max_len\n",
    "        if len(labels_array) > max_len:\n",
    "            labels_array = labels_array[:max_len]\n",
    "        else:\n",
    "            labels_array = labels_array + [-100] * (max_len - len(labels_array))\n",
    "        \n",
    "        input_ids.append(torch.tensor(combined_tokens))\n",
    "        labels.append(torch.tensor(labels_array))\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids_tensor = torch.stack(input_ids)\n",
    "    labels_tensor = torch.stack(labels)\n",
    "    \n",
    "    return input_ids_tensor, labels_tensor\n",
    "\n",
    "# Usage\n",
    "train_inp, train_out = prepare_data(inp_train, out_train, 1024)\n",
    "test_inp, test_out = prepare_data(inp_test, out_test, 1024)\n",
    "# print(train_inp[0])\n",
    "# print(train_out[0][1023])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chetan/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Modified LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                      # Increased rank\n",
    "    lora_alpha=32,            # Increased scaling\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Apply to both attention and projection\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Get LoRA model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return average_rouge1, average_rouge2, average_rougeL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/72 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 108.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1645\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1647\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1648\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1649\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1650\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1651\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1652\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1653\u001b[0m         )\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1316\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1317\u001b[0m     input_ids,\n\u001b[1;32m   1318\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1319\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1320\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1321\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1322\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1323\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1324\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1325\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1326\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1327\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1328\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1329\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1330\u001b[0m )\n\u001b[1;32m   1331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1130\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1119\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1120\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         output_attentions,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m   1131\u001b[0m         hidden_states,\n\u001b[1;32m   1132\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m   1133\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1134\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[1;32m   1135\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1136\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1137\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1138\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    578\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 28.06 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 108.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# model = GPT2SoftPrompt(\"gpt2\", num_prompts)\n",
    "\n",
    "# # Freeze GPT-2 model weights\n",
    "# for param in model.gpt2_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# lora_model = lora_model.to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inp, train_out)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "clip_value = 1.0\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)# prompt_id = prompt_id.to(device)\n",
    "train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_rouge1_scores = []\n",
    "    train_rouge2_scores = []\n",
    "    train_rougeL_scores = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "        #     # Convert tensor predictions and references to lists\n",
    "        #     predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "        #     references = summary.squeeze(0).tolist()\n",
    "\n",
    "        #     # Decode predictions and references, ignoring pad tokens\n",
    "        #     decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "        #     decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "        #     rouge1, rouge2, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "        #     train_rouge1_scores.append(rouge1)\n",
    "        #     train_rouge2_scores.append(rouge2)\n",
    "        #     train_rougeL_scores.append(rougeL)\n",
    "\n",
    "\n",
    "        # # # Calculate average training loss\n",
    "        # avg_train_loss = total_loss / len(train_inp)  # Use len(train_inp) for average\n",
    "        # train_losses.append(avg_train_loss)\n",
    "        # avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
    "        # avg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\n",
    "        # avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
    "\n",
    "        # print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
    "        # print(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\n",
    "        # print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
    "\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        # model.eval()\n",
    "        # total_val_loss = 0\n",
    "        # correct = 0\n",
    "        # total = 0\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for batch in dataloader_val:\n",
    "        #         context_words, target_words = batch\n",
    "        #         context_words = context_words.to(device)\n",
    "        #         target_words = target_words.to(device)\n",
    "\n",
    "        #         outputs = model(context_words, target_words[:, :-1])\n",
    "\n",
    "        #         outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "        #         target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        #         loss = criterion(outputs, target_words_out)\n",
    "\n",
    "        #         total_val_loss += loss.item()\n",
    "\n",
    "        #         _, predicted = torch.max(outputs, 1)\n",
    "        #         total += target_words_out.size(0)\n",
    "        #         correct += (predicted == target_words_out).sum().item()\n",
    "\n",
    "\n",
    "        # avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "        # val_losses.append(avg_val_loss)\n",
    "        # accuracy = 100 * correct / total\n",
    "\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},')\n",
    "            #    Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archaeologists have uncovered the complete skeleton of a 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683. in a country where cows dominate the rural landscape, the discovery in an austrian cellar shocked scientists. the researchers described it as a 'sunken ship in the desert'. archaeologists have uncovered the complete skeleton of an 'alien' 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683 . genetic analysis of the beast show that it was a bactrian-dromedary hybrid – a breed popular in the ottoman army at the time. 'the partly excavated skeleton was at first suspected to be a large horse or cattle,' said alfred galik, a researcher at the university of veterinary medicine vienna. 'but one look at the cervical vertebrae, the lower jaw and the metacarpal bones immediately revealed that this was a camel.' the camel was male, around seven years old and most likely castrated. the camel was male, around seven years old and most likely castrated. along with dna evidence, the shape of the animal's skull indicated it was a hybrid . the cross-breed camel had been most likely used as a riding and transport animal 683. pictured are various views of the camel's metacarpus, which is part of its legs . the battle that took place on 11 september 1683 after vienna had been seiged  it was won  historians claim the battle marked the turning-point in the ottoman–habsburg wars, a 300-year struggle between the holy roman empire and the ottoman empire . the loot that fell into the hands of the holy league troops and the viennese was large. king john sobieski vividly described in a letter to his wife a few days after the battle: . 'ours are treasures unheard of... tents, sheep, cattle and no small number of camels... it is victory as nobody ever knew before.' galik and his team also said the cross-breed camel had been most likely used as a\n"
     ]
    }
   ],
   "source": [
    "lora_model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"archaeologists have uncovered the complete skeleton of a 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683. in a country where cows dominate the rural landscape, the discovery in an austrian cellar shocked scientists. the researchers described it as a 'sunken ship in the desert'. archaeologists have uncovered the complete skeleton of an 'alien' 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683 . genetic analysis of the beast show that it was a bactrian-dromedary hybrid – a breed popular in the ottoman army at the time. 'the partly excavated skeleton was at first suspected to be a large horse or cattle,' said alfred galik, a researcher at the university of veterinary medicine vienna. 'but one look at the cervical vertebrae, the lower jaw and the metacarpal bones immediately revealed that this was a camel.' the camel was male, around seven years old and most likely castrated. the camel was male, around seven years old and most likely castrated. along with dna evidence, the shape of the animal's skull indicated it was a hybrid . the cross-breed camel had been most likely used as a riding and transport animal 683. pictured are various views of the camel's metacarpus, which is part of its legs . the battle that took place on 11 september 1683 after vienna had been seiged  it was won  historians claim the battle marked the turning-point in the ottoman–habsburg wars, a 300-year struggle between the holy roman empire and the ottoman empire . the loot that fell into the hands of the holy league troops and the viennese was large. king john sobieski vividly described in a letter to his wife a few days after the battle: . 'ours are treasures unheard of... tents, sheep, cattle and no small number of camels... it is victory as nobody ever knew before.' galik and his team also said the cross-breed camel had been most likely used as a riding and transport animal 683. the remarkable find was made during an archaeological dig that took place amid preparations for a new shopping centre in the town. if modern-day scientists were stumped  it would have been an even greater shock for residents of 17th-century tulln. 'the animal was certainly exotic for the people of tulln. they probably didn't know what to feed it or whether one could eat it,' galik said. while roman-era camel bones occasionally surface in austria, serbia and belgium, the tulln discovery was the first complete camel skeleton to emerge in central europe. 'this means that the animal was not killed and then butchered. it may have been acquired as part of an exchange,' said galik . the remarkable find was made during an archaeological dig that took place amid preparations for a new shopping centre in the town. pictured the camel's shoulder blades . in addition to horses, the ottoman army also used camels for transportation and as riding animals. in cases of scarcity, the soldiers also ate the animal's flesh. the camel was likely used in the 1683 battle of vienna, which took place on 11 september after vienna had been seiged  it was won  historians claim the battle marked the turning-point in the ottoman–habsburg wars, a 300-year struggle between the holy roman empire and the ottoman empire . the loot that fell into the hands of the holy league troops and the viennese was large. king john sobieski vividly described in a letter to his wife a few days after the battle: . 'ours are treasures unheard of... tents, sheep, cattle and no small number of camels... it is victory as nobody ever knew before.' in a country where cows dominate the rural landscape, the discovery in an austrian cellar shocked scientists. the researchers described it as a 'sunken ship in the desert'. the find was made it the town of tulln . genetic analysis of the beast show that it was a bactrian (pictured)-dromedary hybrid – a breed popular in the ottoman army at the time .\"\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=512)\n",
    "input_ids = torch.tensor(input_ids)\n",
    "input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = lora_model.generate(\n",
    "        input_ids,\n",
    "        max_length=513,  # Adjust as needed for summary length\n",
    "        num_beams=5,     # Beam search for better generation quality\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "# predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(predicted_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
