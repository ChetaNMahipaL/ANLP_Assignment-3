{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYfscHa30RAI"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciNx1Bfo0RAJ",
        "outputId": "62a794fd-a82f-4d68-f978-a9309c3e9002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/chetan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "# from rouge_score import rouge_scorer\n",
        "import string\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "# !pip install peft\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import csv\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J44_jlto0RAK"
      },
      "source": [
        "### **Dataset Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KlMdyGP-0RAK"
      },
      "outputs": [],
      "source": [
        "def load_csv(file_path):\n",
        "    data = []\n",
        "\n",
        "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "\n",
        "        for row in reader:\n",
        "            data.append({\n",
        "                'id': row['id'],\n",
        "                'article': row['article'],\n",
        "                'highlights': row['highlights']\n",
        "            })\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = load_csv('./train.csv')\n",
        "test_data = load_csv('./test.csv')\n",
        "val_data = load_csv('./validation.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VM03J6nk0RAK"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    pattern = r\"(?i)(PUBLISHED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4}\\s*.\\s*\\|\\s*.\\s*UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
        "              r\"(By\\s*.\\s*[A-Za-z\\s]+.)|\" \\\n",
        "              r\"(\\([A-Za-z\\s]*CNN\\)\\s*--)|\" \\\n",
        "              r\"(Follow\\s*@@[A-Za-z0-9_]+)|\" \\\n",
        "              r\"(UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
        "              r\"(Last\\s*updated\\s*at\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*on\\s*\\d{1,2}(st|nd|rd|th)\\s*\\w+\\s\\d{4}\\s*.)|\" \\\n",
        "              r\"(\\(CNN\\))\"\n",
        "\n",
        "    cleaned_text = re.sub(pattern, '', text).strip()\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_articles(data):\n",
        "    for entry in data:\n",
        "        entry['article'] = clean_text(entry['article'])\n",
        "        entry['highlights'] = clean_text(entry['highlights'])\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_csv(file_path, cleaned_data):\n",
        "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
        "        writer=writer\n",
        "        writer.writeheader()\n",
        "\n",
        "        for row in cleaned_data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# train_data = clean_articles(train_data)\n",
        "# test_data = clean_articles(test_data)\n",
        "# val_data = clean_articles(val_data)\n",
        "\n",
        "# write_csv(\"./Cleaned_Dataset/train.csv\", train_data)\n",
        "# write_csv(\"./Cleaned_Dataset/test.csv\", test_data)\n",
        "# write_csv(\"./Cleaned_Dataset/validation.csv\", val_data)\n",
        "\n",
        "# def write_csv(file_path, cleaned_data, percentage=1):\n",
        "#     # Calculate how many rows to write based on the percentage\n",
        "#     data_size = len(cleaned_data)\n",
        "#     num_rows = data_size * percentage // 100\n",
        "\n",
        "#     with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
        "#         writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
        "#         writer.writeheader()\n",
        "\n",
        "#         # Write only the first 'num_rows' rows of the data\n",
        "#         for row in cleaned_data[:num_rows]:\n",
        "#             writer.writerow(row)\n",
        "\n",
        "# # Assuming train_data, test_data, val_data are your datasets\n",
        "# write_csv(\"./train.csv\", train_data)\n",
        "# write_csv(\"./test.csv\", test_data)\n",
        "# write_csv(\"./validation.csv\", val_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ohVCRSB0RAL"
      },
      "source": [
        "### **Intialize special Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a_udztL0RAL",
        "outputId": "36079a16-3adf-4adb-9d9a-5034b5056aff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chetan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# import tokenizer for padding\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\n",
        "    'additional_special_tokens': ['[SUM]']  # Add summary token\n",
        "})\n",
        "\n",
        "# # Initializing Pad tokens\n",
        "# pad_token = tokenizer.eos_token_id\n",
        "# print(pad_token)\n",
        "# tokenizer.add_tokens([pad_token])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70LtCOh0RAL"
      },
      "source": [
        "### **Tokenizing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nerWP41W0RAL",
        "outputId": "cfbcde3e-a74e-4839-e197-80dc9838d87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "287\n"
          ]
        }
      ],
      "source": [
        "def convertCSV(data):\n",
        "    inp = []\n",
        "    out = []\n",
        "    for row in data:\n",
        "        inp.append(row['article'])\n",
        "        out.append(row['highlights'])\n",
        "\n",
        "    return inp, out\n",
        "\n",
        "inp_train, out_train = convertCSV(train_data)\n",
        "inp_test, out_test = convertCSV(test_data)\n",
        "\n",
        "train_size = int(0.1 * len(inp_train))\n",
        "inp_train_10 = inp_train[:train_size]\n",
        "out_train_10 = out_train[:train_size]\n",
        "\n",
        "test_size = int(0.1 * len(inp_test))\n",
        "inp_test_10 = inp_test[:test_size]\n",
        "out_test_10 = out_test[:test_size]\n",
        "\n",
        "# print(inp_train[0])\n",
        "def remove_punctuation(tokenized_sentence):\n",
        "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
        "#Using NLTK Tokenize\n",
        "\n",
        "inp_train = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_train_10]\n",
        "inp_test = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_test_10]\n",
        "out_train = [remove_punctuation(word_tokenize(sentence)) for sentence in out_train_10]\n",
        "out_test = [remove_punctuation(word_tokenize(sentence)) for sentence in out_test_10]\n",
        "\n",
        "print(len(inp_train))\n",
        "# print(out_train[0])\n",
        "\n",
        "max_len = 0\n",
        "for i in inp_train:\n",
        "    # if max_len < len(i):\n",
        "    max_len += len(i)\n",
        "\n",
        "# print(max_len/len(inp_train))\n",
        "\n",
        "# def tokenize(data,max_len = 1000):\n",
        "def prepare_data(articles, summaries, max_len=1024):\n",
        "    input_ids = []\n",
        "    labels = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for article, summary in zip(articles, summaries):\n",
        "        # Format: Article [SUM] Summary\n",
        "        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_len-200)\n",
        "        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=200)\n",
        "\n",
        "        # Create input sequence: article [SUM] summary\n",
        "        input_sequence = (\n",
        "            article_tokens +\n",
        "            [tokenizer.additional_special_tokens_ids[0]] +  # [SUM] token\n",
        "            summary_tokens\n",
        "        )\n",
        "\n",
        "        # Ensure we don't exceed max_len\n",
        "        if len(input_sequence) > max_len:\n",
        "            input_sequence = input_sequence[:max_len]\n",
        "\n",
        "        # Create attention mask for the truncated sequence\n",
        "        attention_mask = [1] * len(input_sequence)\n",
        "\n",
        "        # Create labels array of the same length as input_sequence\n",
        "        labels_array = (\n",
        "            [-100] * len(article_tokens) +  # Don't compute loss for article tokens\n",
        "            [-100] * 1 +  # Don't compute loss for [SUM] token\n",
        "            summary_tokens  # Compute loss for summary tokens\n",
        "        )\n",
        "\n",
        "        # Truncate labels if needed\n",
        "        if len(labels_array) > max_len:\n",
        "            labels_array = labels_array[:max_len]\n",
        "\n",
        "        # Pad all sequences to max_len\n",
        "        padding_length = max_len - len(input_sequence)\n",
        "\n",
        "        input_sequence = input_sequence + [tokenizer.pad_token_id] * padding_length\n",
        "        attention_mask = attention_mask + [0] * padding_length\n",
        "        labels_array = labels_array + [-100] * (max_len - len(labels_array))\n",
        "\n",
        "        # Verify lengths before adding to lists\n",
        "        if len(input_sequence) == max_len and len(attention_mask) == max_len and len(labels_array) == max_len:\n",
        "            input_ids.append(torch.tensor(input_sequence))\n",
        "            attention_masks.append(torch.tensor(attention_mask))\n",
        "            labels.append(torch.tensor(labels_array))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids_tensor = torch.stack(input_ids)\n",
        "    attention_masks_tensor = torch.stack(attention_masks)\n",
        "    labels_tensor = torch.stack(labels)\n",
        "\n",
        "    # print(f\"Input shape: {input_ids_tensor.shape}\")\n",
        "    # print(f\"Attention mask shape: {attention_masks_tensor.shape}\")\n",
        "    # print(f\"Labels shape: {labels_tensor.shape}\")\n",
        "\n",
        "    return input_ids_tensor, attention_masks_tensor, labels_tensor\n",
        "\n",
        "# Usage\n",
        "train_inp, masks_train, train_out = prepare_data(inp_train, out_train, 1024)\n",
        "test_inp, masks_test, test_out = prepare_data(inp_test, out_test, 1024)\n",
        "\n",
        "# Usage\n",
        "train_inp, masks_train, train_out = prepare_data(inp_train, out_train, 1024)\n",
        "test_inp, masks_test, test_out = prepare_data(inp_test, out_test, 1024)\n",
        "# print(train_inp[0])\n",
        "# print(train_out[0][1023])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JON48WMF0RAL"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J859Crnj0RAL",
        "outputId": "cc0e0889-f44f-4817-9424-465a76042afd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chetan/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50258, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=3072)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Get LoRA model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzuaOLio0RAL"
      },
      "source": [
        "### **Evaluation Metric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lq2xOU4_0RAL"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge_scores(generated_answers, ground_truth):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
        "    for gen, ref in zip(generated_answers, ground_truth):\n",
        "        scores = scorer.score(gen, ref)\n",
        "        total_rouge1 += scores['rouge1'].fmeasure\n",
        "        total_rouge2 += scores['rouge2'].fmeasure\n",
        "        total_rougeL += scores['rougeL'].fmeasure\n",
        "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
        "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
        "    average_rougeL = total_rougeL / len(generated_answers)\n",
        "    return average_rouge1, average_rouge2, average_rougeL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR76OIFR0RAL"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQIkHoTM0RAM",
        "outputId": "a9d23966-ea50-491c-be93-cb157ca10165"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 72/72 [01:28<00:00,  1.23s/it, loss=5.27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Average Loss: 18.3177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 72/72 [01:24<00:00,  1.17s/it, loss=5.61]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Average Loss: 5.2630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 72/72 [01:24<00:00,  1.18s/it, loss=4.51]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Average Loss: 4.9562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# model = GPT2SoftPrompt(\"gpt2\", num_prompts)\n",
        "\n",
        "# # Freeze GPT-2 model weights\n",
        "# for param in model.gpt2_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "# lora_model = lora_model.to(device)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_inp, masks_train, train_out)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "num_epochs = 3\n",
        "learning_rate = 5e-5\n",
        "clip_value = 1.0\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)# prompt_id = prompt_id.to(device)\n",
        "train_losses = []\n",
        "# val_losses = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_rouge1_scores = []\n",
        "    train_rouge2_scores = []\n",
        "    train_rougeL_scores = []\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for batch in progress_bar:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
        "        #     # Convert tensor predictions and references to lists\n",
        "        #     predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
        "        #     references = summary.squeeze(0).tolist()\n",
        "\n",
        "        #     # Decode predictions and references, ignoring pad tokens\n",
        "        #     decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
        "        #     decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
        "\n",
        "        #     rouge1, rouge2, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
        "        #     train_rouge1_scores.append(rouge1)\n",
        "        #     train_rouge2_scores.append(rouge2)\n",
        "        #     train_rougeL_scores.append(rougeL)\n",
        "\n",
        "\n",
        "        # # # Calculate average training loss\n",
        "        # avg_train_loss = total_loss / len(train_inp)  # Use len(train_inp) for average\n",
        "        # train_losses.append(avg_train_loss)\n",
        "        # avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
        "        # avg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\n",
        "        # avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
        "\n",
        "        # print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
        "        # print(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\n",
        "        # print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
        "\n",
        "\n",
        "\n",
        "        # Validation loop\n",
        "        # model.eval()\n",
        "        # total_val_loss = 0\n",
        "        # correct = 0\n",
        "        # total = 0\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     for batch in dataloader_val:\n",
        "        #         context_words, target_words = batch\n",
        "        #         context_words = context_words.to(device)\n",
        "        #         target_words = target_words.to(device)\n",
        "\n",
        "        #         outputs = model(context_words, target_words[:, :-1])\n",
        "\n",
        "        #         outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "        #         target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        #         loss = criterion(outputs, target_words_out)\n",
        "\n",
        "        #         total_val_loss += loss.item()\n",
        "\n",
        "        #         _, predicted = torch.max(outputs, 1)\n",
        "        #         total += target_words_out.size(0)\n",
        "        #         correct += (predicted == target_words_out).sum().item()\n",
        "\n",
        "\n",
        "        # avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "        # val_losses.append(avg_val_loss)\n",
        "        # accuracy = 100 * correct / total\n",
        "\n",
        "        # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},')\n",
        "            #    Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOFBmIOL0RAM",
        "outputId": "79b33fa5-b99b-49e3-ab98-0bcc502c7239"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "triumph as\n"
          ]
        }
      ],
      "source": [
        "# lora_model.eval()\n",
        "\n",
        "# # Input text for summarization\n",
        "article = \"archaeologists have uncovered the complete skeleton of a 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683. in a country where cows dominate the rural landscape, the discovery in an austrian cellar shocked scientists. the researchers described it as a 'sunken ship in the desert'. archaeologists have uncovered the complete skeleton of an 'alien' 17th-century camel that was likely used in the second ottoman-habsburg war. they believe the camel - the first intact camel skeleton found in central europe - may have been left in the town of tulln for trading after the siege of vienna in 1683 . genetic analysis of the beast show that it was a bactrian-dromedary hybrid – a breed popular in the ottoman army at the time. 'the partly excavated skeleton was at first suspected to be a large horse or cattle,' said alfred galik, a researcher at the university of veterinary medicine vienna. 'but one look at the cervical vertebrae, the lower jaw and the metacarpal bones immediately revealed that this was a camel.' the camel was male, around seven years old and most likely castrated. the camel was male, around seven years old and most likely castrated. along with dna evidence, the shape of the animal's skull indicated it was a hybrid . the cross-breed camel had been most likely used as a riding and transport animal 683. pictured are various views of the camel's metacarpus, which is part of its legs . the battle that took place on 11 september 1683 after vienna had been seiged  it was won  historians claim the battle marked the turning-point in the ottoman–habsburg wars, a 300-year struggle between the holy roman empire and the ottoman empire . the loot that fell into the hands of the holy league troops and the viennese was large. king john sobieski vividly described in a letter to his wife a few days after the battle: . 'ours are treasures unheard of... tents, sheep, cattle and no small number of camels... it is victory as nobody ever knew before.' galik and his team also said the cross-breed camel had been most likely used as a riding and transport animal 683. the remarkable find was made during an archaeological dig that took place amid preparations for a new shopping centre in the town. if modern-day scientists were stumped  it would have been an even greater shock for residents of 17th-century tulln. 'the animal was certainly exotic for the people of tulln. they probably didn't know what to feed it or whether one could eat it,' galik said. while roman-era camel bones occasionally surface in austria, serbia and belgium, the tulln discovery was the first complete camel skeleton to emerge in central europe. 'this means that the animal was not killed and then butchered. it may have been acquired as part of an exchange,' said galik . the remarkable find was made during an archaeological dig that took place amid preparations for a new shopping centre in the town. pictured the camel's shoulder blades . in addition to horses, the ottoman army also used camels for transportation and as riding animals. in cases of scarcity, the soldiers also ate the animal's flesh. the camel was likely used in the 1683 battle of vienna, which took place on 11 september after vienna had been seiged  it was won  historians claim the battle marked the turning-point in the ottoman–habsburg wars, a 300-year struggle between the holy roman empire and the ottoman empire . the loot that fell into the hands of the holy league troops and the viennese was large. king john sobieski vividly described in a letter to his wife a few days after the battle: . 'ours are treasures unheard of... tents, sheep, cattle and no small number of camels... it is victory as nobody ever knew before.' in a country where cows dominate the rural landscape, the discovery in an austrian cellar shocked scientists. the researchers described it as a 'sunken ship in the desert'. the find was made it the town of tulln . genetic analysis of the beast show that it was a bactrian (pictured)-dromedary hybrid – a breed popular in the ottoman army at the time .\"\n",
        "# # Tokenize and encode the input text\n",
        "# input_ids = tokenizer.encode(input_text, truncation=True, max_length=512)\n",
        "# input_ids = torch.tensor(input_ids)\n",
        "# input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
        "# input_ids = input_ids.to(device)\n",
        "\n",
        "# # Convert the input_ids to a PyTorch tensor\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     generated_ids = lora_model.generate(\n",
        "#         input_ids,\n",
        "#         max_length=513,  # Adjust as needed for summary length\n",
        "#         num_beams=5,     # Beam search for better generation quality\n",
        "#         early_stopping=True\n",
        "#     )\n",
        "\n",
        "\n",
        "# # Get the token IDs with the highest probability for each position\n",
        "# # predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
        "\n",
        "# # Convert token IDs into words using the tokenizer\n",
        "# predicted_tokens = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "# print(predicted_tokens)\n",
        "model.eval()\n",
        "\n",
        "# Encode article\n",
        "inputs = tokenizer.encode(\n",
        "    article,\n",
        "    truncation=True,\n",
        "    max_length=1024-150,\n",
        "    return_tensors='pt'\n",
        ").to(device)\n",
        "\n",
        "# Add summary token\n",
        "input_ids = torch.cat([\n",
        "    inputs,\n",
        "    torch.tensor([[tokenizer.additional_special_tokens_ids[0]]]).to(device)\n",
        "], dim=1)\n",
        "\n",
        "# Generate summary\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=input_ids.shape[1] + 150,\n",
        "        min_length=input_ids.shape[1] + 30,\n",
        "        num_beams=4,\n",
        "        length_penalty=2.0,\n",
        "        no_repeat_ngram_size=3,\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode and extract summary\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "summary = decoded.split('[SUM]')[1].strip()\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
