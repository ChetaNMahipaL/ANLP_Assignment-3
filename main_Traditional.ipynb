{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chetan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chetan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from rouge_score import rouge_scorer\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            data.append({\n",
    "                'id': row['id'],\n",
    "                'article': row['article'],\n",
    "                'highlights': row['highlights']\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_csv('./Cleaned_Dataset/train.csv')\n",
    "test_data = load_csv('./Cleaned_Dataset/test.csv')\n",
    "val_data = load_csv('./Cleaned_Dataset/validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    pattern = r\"(?i)(PUBLISHED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4}\\s*.\\s*\\|\\s*.\\s*UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(By\\s*.\\s*[A-Za-z\\s]+.)|\" \\\n",
    "              r\"(\\([A-Za-z\\s]*CNN\\)\\s*--)|\" \\\n",
    "              r\"(Follow\\s*@@[A-Za-z0-9_]+)|\" \\\n",
    "              r\"(UPDATED:\\s*.\\s*\\d{1,2}:\\d{2}\\s*(EST|PST),\\s*\\d{1,2}\\s\\w+\\s\\d{4})|\" \\\n",
    "              r\"(Last\\s*updated\\s*at\\s*\\d{1,2}:\\d{2}\\s*(AM|PM)\\s*on\\s*\\d{1,2}(st|nd|rd|th)\\s*\\w+\\s\\d{4}\\s*.)|\" \\\n",
    "              r\"(\\(CNN\\))\"\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text).strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_articles(data):\n",
    "    for entry in data:\n",
    "        entry['article'] = clean_text(entry['article'])\n",
    "        entry['highlights'] = clean_text(entry['highlights'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def write_csv(file_path, cleaned_data):\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "        writer=writer\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in cleaned_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# train_data = clean_articles(train_data)\n",
    "# test_data = clean_articles(test_data)\n",
    "# val_data = clean_articles(val_data)\n",
    "\n",
    "# write_csv(\"./Cleaned_Dataset/train.csv\", train_data)\n",
    "# write_csv(\"./Cleaned_Dataset/test.csv\", test_data)\n",
    "# write_csv(\"./Cleaned_Dataset/validation.csv\", val_data)\n",
    "            \n",
    "# def write_csv(file_path, cleaned_data, percentage=1):\n",
    "#     # Calculate how many rows to write based on the percentage\n",
    "#     data_size = len(cleaned_data)\n",
    "#     num_rows = data_size * percentage // 100\n",
    "\n",
    "#     with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=['id', 'article', 'highlights'])\n",
    "#         writer.writeheader()\n",
    "        \n",
    "#         # Write only the first 'num_rows' rows of the data\n",
    "#         for row in cleaned_data[:num_rows]:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# # Assuming train_data, test_data, val_data are your datasets\n",
    "# write_csv(\"./train.csv\", train_data)\n",
    "# write_csv(\"./test.csv\", test_data)\n",
    "# write_csv(\"./validation.csv\", val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intialize special Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEP> token ID: 50257\n",
      "<PAD> token ID: 50258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chetan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import tokenizer for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add the special <SEP> token to the tokenizer\n",
    "special_tokens_dict = {'sep_token': '<SEP>','pad_token': '<PAD>'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# Verify that the <SEP> token has been added\n",
    "sep_token_id = tokenizer.convert_tokens_to_ids('<SEP>')\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids('<PAD>')\n",
    "print(f\"<SEP> token ID: {sep_token_id}\")\n",
    "print(f\"<PAD> token ID: {pad_token_id}\")\n",
    "\n",
    "\n",
    "# Initializing Pad tokens\n",
    "pad_token = tokenizer.eos_token_id\n",
    "# tokenizer.add_tokens([pad_token])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n",
      "670.3214285714286\n"
     ]
    }
   ],
   "source": [
    "def convertCSV(data):\n",
    "    inp = []\n",
    "    out = []\n",
    "    for row in data:\n",
    "        inp.append(row['article'])\n",
    "        out.append(row['highlights'])\n",
    "    \n",
    "    return inp, out\n",
    "\n",
    "inp_train, out_train = convertCSV(train_data)\n",
    "inp_test, out_test = convertCSV(test_data)\n",
    "\n",
    "train_size = int(0.004 * len(inp_train))\n",
    "inp_train_10 = inp_train[:train_size]\n",
    "out_train_10 = out_train[:train_size]\n",
    "\n",
    "test_size = int(0.004 * len(inp_test))\n",
    "inp_test_10 = inp_test[:test_size]\n",
    "out_test_10 = out_test[:test_size]\n",
    "\n",
    "# print(inp_train[0])\n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "#Using NLTK Tokenize\n",
    "\n",
    "inp_train = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_train_10]\n",
    "inp_test = [remove_punctuation(word_tokenize(sentence)) for sentence in inp_test_10]\n",
    "out_train = [remove_punctuation(word_tokenize(sentence)) for sentence in out_train_10]\n",
    "out_test = [remove_punctuation(word_tokenize(sentence)) for sentence in out_test_10]\n",
    "\n",
    "print(len(inp_train))\n",
    "# print(out_train[0])\n",
    "\n",
    "max_len = 0\n",
    "for i in inp_train:\n",
    "    # if max_len < len(i):\n",
    "    max_len += len(i)\n",
    "    \n",
    "print(max_len/len(inp_train))\n",
    "\n",
    "# def tokenize(data,max_len = 1000):\n",
    "\n",
    "# def prepare_data(sentences,pad, max_len=1024):\n",
    "#     all_indices = []\n",
    "#     for _, sentence in enumerate(sentences):\n",
    "\n",
    "#         tokens = tokenizer.encode(sentence,truncation=True,max_length=max_len)\n",
    "#         padded_tokens = torch.tensor(tokens + [pad] * (max_len - len(tokens)))\n",
    "        \n",
    "#         all_indices.append(padded_tokens)\n",
    "        \n",
    "#     return all_indices\n",
    "\n",
    "\n",
    "# train_inp = prepare_data(inp_train,pad_token,1024)\n",
    "# test_inp = prepare_data(inp_test,pad_token,1024)\n",
    "# train_out = prepare_data(out_train,pad_token,1024)\n",
    "# test_out = prepare_data(out_test,pad_token, 1024)\n",
    "def prepare_data(articles, summaries, pad_token_id, max_len=1024):\n",
    "    all_data = []\n",
    "    for article, summary in zip(articles, summaries):\n",
    "        input_text = f\"{article} <SEP> {summary}\"\n",
    "        \n",
    "        tokens = tokenizer.encode(input_text, truncation=True, max_length=max_len)\n",
    "        \n",
    "        input_ids = tokens[:-1]\n",
    "        labels = tokens[1:]\n",
    "        \n",
    "        padded_input_ids = input_ids + [pad_token_id] * (max_len - 1 - len(input_ids))\n",
    "        padded_labels = labels + [pad_token_id] * (max_len - 1 - len(labels))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_tensor = torch.tensor(padded_input_ids)\n",
    "        label_tensor = torch.tensor(padded_labels)\n",
    "        \n",
    "        all_data.append((input_tensor, label_tensor))\n",
    "    \n",
    "    return all_data\n",
    "# Prepare the training and testing datasets\n",
    "train_data = prepare_data(inp_train, out_train, pad_token_id, 1024)\n",
    "test_data = prepare_data(inp_test, out_test, pad_token_id, 1024)\n",
    "\n",
    "# print(train_inp[0])\n",
    "# print(train_out[0][1023])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SummarizationFineTune(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.gpt2.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in self.gpt2.transformer.h[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        if labels is not None:\n",
    "            outputs = self.gpt2(input_ids=input_ids, labels=labels)\n",
    "        else:\n",
    "            outputs = self.gpt2(input_ids=input_ids)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    return average_rouge1, average_rouge2, average_rougeL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2SummarizationFineTune(\"gpt2\")\n",
    "\n",
    "# # # Freeze GPT-2 model weights\n",
    "# # for param in model.gpt2_model.parameters():\n",
    "# #     param.requires_grad = False\n",
    "# model = model.to(device)\n",
    "\n",
    "# num_epochs = 1\n",
    "# learning_rate = 2e-3\n",
    "# clip_value = 1.0\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# prompt_id = prompt_id.to(device)\n",
    "# train_losses = []\n",
    "# # val_losses = []\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_rouge1_scores = []\n",
    "#     train_rouge2_scores = []\n",
    "#     train_rougeL_scores = []\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     with tqdm(enumerate(zip(train_inp, train_out)), total=len(train_inp), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as progress:\n",
    "#         train_percentage_matched = 0\n",
    "#         train_percentage_matched_ct = 0\n",
    "#         for _, (article, summary) in progress:\n",
    "#             context_words = article.to(device)\n",
    "#             target_words = summary.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(context_words, labels=target_words)\n",
    "#             logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "#             # Reshape outputs and targets for loss calculation\n",
    "#             # outputs = outputs.view(-1, outputs.size(-1))\n",
    "#             # target_words_out = target_words[:, 1:].view(-1)\n",
    "\n",
    "#             # Calculate loss\n",
    "#             loss = criterion(outputs.logits, target_words)\n",
    "\n",
    "#             # Backward pass and optimization step\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "#             # Convert tensor predictions and references to lists\n",
    "#             predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "#             references = summary.squeeze(0).tolist()\n",
    "\n",
    "#             # Decode predictions and references, ignoring pad tokens\n",
    "#             decoded_predictions = tokenizer.decode([token for token in predictions if token != pad_token])\n",
    "#             decoded_references = tokenizer.decode([token for token in references if token != pad_token])\n",
    "\n",
    "#             rouge1, rouge2, rougeL = calculate_rouge_scores([decoded_predictions], [decoded_references])\n",
    "#             train_rouge1_scores.append(rouge1)\n",
    "#             train_rouge2_scores.append(rouge2)\n",
    "#             train_rougeL_scores.append(rougeL)\n",
    "\n",
    "\n",
    "#         # # Calculate average training loss\n",
    "#         avg_train_loss = total_loss / len(train_inp)  # Use len(train_inp) for average\n",
    "#         train_losses.append(avg_train_loss)\n",
    "#         avg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\n",
    "#         avg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\n",
    "#         avg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n",
    "\n",
    "#         print(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\n",
    "#         print(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\n",
    "#         print(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n",
    "\n",
    "\n",
    "\n",
    "#         # Validation loop\n",
    "#         # model.eval()\n",
    "#         # total_val_loss = 0\n",
    "#         # correct = 0\n",
    "#         # total = 0\n",
    "\n",
    "#         # with torch.no_grad():\n",
    "#         #     for batch in dataloader_val:\n",
    "#         #         context_words, target_words = batch\n",
    "#         #         context_words = context_words.to(device)\n",
    "#         #         target_words = target_words.to(device)\n",
    "\n",
    "#         #         outputs = model(context_words, target_words[:, :-1])\n",
    "\n",
    "#         #         outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "#         #         target_words_out = target_words[:, 1:].contiguous().view(-1)\n",
    "\n",
    "#         #         loss = criterion(outputs, target_words_out)\n",
    "\n",
    "#         #         total_val_loss += loss.item()\n",
    "\n",
    "#         #         _, predicted = torch.max(outputs, 1)\n",
    "#         #         total += target_words_out.size(0)\n",
    "#         #         correct += (predicted == target_words_out).sum().item()\n",
    "\n",
    "\n",
    "#         # avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "#         # val_losses.append(avg_val_loss)\n",
    "#         # accuracy = 100 * correct / total\n",
    "\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},')\n",
    "#             #    Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, num_epochs, device, learning_rate=2e-3, pad_token=None):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\") as progress:\n",
    "            for batch_idx, (input_ids, labels) in enumerate(progress):\n",
    "                input_ids = input_ids.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Reshape logits and labels for loss calculation\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                               shift_labels.view(-1))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                progress.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# # Input text for summarization\n",
    "# input_text = \"the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda (pictured) of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it's important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota (pictured) is where the bishop is located .\"\n",
    "# # Tokenize and encode the input text\n",
    "# input_ids = tokenizer.encode(input_text, truncation=True, max_length=1023)\n",
    "\n",
    "# # Convert the input_ids to a PyTorch tensor\n",
    "# input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# # Generate a summary\n",
    "# with torch.no_grad():\n",
    "#     # Assuming single prompt\n",
    "#     outputs = model(input_ids.to(device),prompt_id)\n",
    "#     pred_logits = outputs.logits\n",
    "#     # print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# # Get the token IDs with the highest probability for each position\n",
    "# predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# # Convert token IDs into words using the tokenizer\n",
    "# predicted_tokens = tokenizer.decode([token for token in predicted_token_ids if token != pad_token])\n",
    "# print(predicted_tokens)\n",
    "\n",
    "def generate_summary(model, article, tokenizer, device, max_length=150):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input text with SEP token\n",
    "    input_text = f\"{article} <SEP>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        outputs = model.gpt2.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            no_repeat_ngram_size=3,\n",
    "            # Stop at <SEP> token if encountered again\n",
    "            eos_token_id=sep_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated summary\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract only the summary part (after <SEP>)\n",
    "    summary = generated_text.split(\"<SEP>\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/287 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 39.31 MiB is free. Including non-PyTorch memory, this process has 3.71 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 66.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     17\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     19\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-3\u001b[39m,\n\u001b[1;32m     20\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m     21\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 130\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, num_epochs, device, learning_rate, pad_token)\u001b[0m\n\u001b[1;32m    127\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[1;32m    131\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Reshape logits and labels for loss calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mGPT2SummarizationFineTune.forward\u001b[0;34m(self, input_ids, labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2(input_ids\u001b[38;5;241m=\u001b[39minput_ids, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1316\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1317\u001b[0m     input_ids,\n\u001b[1;32m   1318\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1319\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1320\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1321\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1322\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1323\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1324\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1325\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1326\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1327\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1328\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1329\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1330\u001b[0m )\n\u001b[1;32m   1331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1130\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1119\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1120\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         output_attentions,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m   1131\u001b[0m         hidden_states,\n\u001b[1;32m   1132\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m   1133\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1134\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[1;32m   1135\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1136\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1137\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1138\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    578\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 39.31 MiB is free. Including non-PyTorch memory, this process has 3.71 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 66.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, add the special token to the tokenizer\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<SEP>']})\n",
    "# Resize model embeddings to account for new token\n",
    "\n",
    "# Prepare the data\n",
    "model = GPT2SummarizationFineTune(\"gpt2\")\n",
    "model.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = SummarizationDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    num_epochs=1,\n",
    "    device=device,\n",
    "    learning_rate=2e-3,\n",
    "    pad_token=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# # Generate example\n",
    "# article = \"Your test article here...\"\n",
    "# summary = generate_summary(model, article, tokenizer, device)\n",
    "# print(f\"Generated Summary: {summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
